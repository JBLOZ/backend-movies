version: "3.8"

# version produccion con cpu --profile prod o --profile default
# version dev con cpu --profile dev
# version produccion con cuda --profile prodcuda
# version dev con cuda --profile devcuda

services:
  db:
    image: mariadb:latest
    container_name: mariadb
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: movies
      MYSQL_USER: user
      MYSQL_PASSWORD: password
    ports:
      - "3306:3306"
    volumes:
      - mariadb_data:/var/lib/mysql

  adminer:
    image: adminer:latest
    container_name: adminer
    ports:
      - "8080:8080"
    depends_on:
      - db

  # Aplicación en producción - copia el código fuente en la imagen
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: movies_app
    environment:
      - DB_URL=mysql+pymysql://user:password@db/movies
      - PYTHONPATH=src
    ports:
      - "8000:80"
    volumes:
      - ./logs:/code/logs
    depends_on:
      - db
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: sh -c "rm -rf /code/data && /code/scripts/wait-for-it.sh db:3306 -t 60 -- fastapi run src/main.py --proxy-headers --port 80"
    profiles:
      - prod
      - prodcuda
      - default

  # Versión de desarrollo - monta el código como volumen
  app_dev:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: movies_app_dev
    environment:
      - DB_URL=mysql+pymysql://user:password@db/movies
      - PYTHONPATH=src
      - ENVIRONMENT=dev
    ports:
      - "8000:80"
    volumes:
      - ./src:/code/src 
      - ./logs:/code/logs
    depends_on:
      - db
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: /code/scripts/wait-for-it.sh db:3306 -t 60 -- fastapi dev src/main.py --host 0.0.0.0 --port 80
    profiles:
      - dev
      - devcuda

  # Servicio de inferencia para producción - usa el comando por defecto en Dockerfile.inference
  inference_service_cuda:
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: inference_service
    environment:
      - PYTHONPATH=/code
    ports:
      - "8001:8001"
    volumes:
      - ./logs:/code/logs
    # Configuración opcional para acceso a la GPU - solo se usará si está disponible
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              # Hacemos que el requisito de GPU sea opcional
              options:
                optional: true
    profiles:
      - prodcuda
      
 
  # Servicio de inferencia para desarrollo - sobreescribe el comando
  inference_service_dev_cuda:
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: inference_service_dev
    environment:
      - PYTHONPATH=/code
      - ENVIRONMENT=dev
    ports:
      - "8001:8001"
    volumes:
    # se monta la carpeta entera asique por ese motivo en desarrollo aparecerá en los logs dos veces el log del sentiment analysis
      - ./inference:/code/ia
      - ./logs:/code/logs
    command: fastapi dev ia/inference_service.py --host 0.0.0.0 --port 8001
    # Configuración opcional para acceso a la GPU - solo se usará si está disponible
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              # Hacemos que el requisito de GPU sea opcional
              options:
                optional: true
    profiles:
      - devcuda
  
  inference_service:
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: inference_service
    environment:
      - PYTHONPATH=/code
    ports:
      - "8001:8001"
    volumes:
      - ./logs:/code/logs
    profiles:
      - prod
      - default
  inference_service_dev:
    build:
      context: .
      dockerfile: Dockerfile.inference
    container_name: inference_service_dev
    environment:
      - PYTHONPATH=/code
      - ENVIRONMENT=dev
    ports:
      - "8001:8001"
    volumes:
      - ./inference:/code/ia
      - ./logs:/code/logs
    command: fastapi dev ia/inference_service.py --host 0.0.0.0 --port 8001

    profiles:
      - dev
volumes:
  mariadb_data: